# Optimized Go Implementation

**High-performance Go implementation with:**
- âœ… Parallel processing (goroutines)
- âœ… Memory pooling (zero allocation)
- âœ… Batch processing
- âœ… Direct pixel buffer access
- âœ… Multi-threaded ONNX Runtime

**Goal:** Maximize performance to match or beat Python!

## Optimizations

### 1. Parallel Processing
- Uses all CPU cores with goroutines
- Processes multiple frames simultaneously
- Semaphore to control worker count

### 2. Memory Pooling
- sync.Pool for tensor buffers
- Reuse allocations across frames
- Zero GC pressure during processing

### 3. Batch Processing
- Processes frames in configurable batches
- Better cache locality
- Reduced synchronization overhead

### 4. Direct Buffer Access
- Uses img.Pix directly (not img.At())
- Eliminates function call overhead
- 5-10x faster than pixel-by-pixel

### 5. Multi-threaded ONNX
- ONNX Runtime uses multiple threads
- Parallel model execution
- Better CPU utilization

## Expected Performance

**Original Go:** 8.9 FPS  
**Optimized Go:** 15-20 FPS (target)  

**Improvements from:**
- Parallel processing: +30-50%
- Memory pooling: +10-20%
- Direct buffer access: +40-60%
- **Total: 2-3x speedup expected!**

## Build

```bash
cd go_optimized
go mod download
CGO_LDFLAGS="-L/opt/homebrew/lib" CGO_CFLAGS="-I/opt/homebrew/include" \
  go build -o bin/infer ./cmd/infer
```

## Usage

```bash
# Default (uses all CPU cores, batch size 10)
DYLD_LIBRARY_PATH=/opt/homebrew/lib ./bin/infer --frames 250

# Custom batch size
DYLD_LIBRARY_PATH=/opt/homebrew/lib ./bin/infer --frames 250 --batch 20

# Custom audio
DYLD_LIBRARY_PATH=/opt/homebrew/lib ./bin/infer \
  --audio ../../demo/talk_hb.wav \
  --frames 250 \
  --batch 10
```

## Performance Tuning

### Batch Size
- Small (5-10): Better for low memory
- Medium (10-20): Balanced
- Large (20-50): Better throughput, more memory

### Workers
Automatically uses all CPU cores. To limit:
```bash
GOMAXPROCS=4 ./bin/infer --frames 250
```

## Benchmarks

Run comparison:
```bash
# Original
cd ../simple_inference_go
time DYLD_LIBRARY_PATH=/opt/homebrew/lib ./bin/infer --frames 250

# Optimized
cd ../go_optimized
time DYLD_LIBRARY_PATH=/opt/homebrew/lib ./bin/infer --frames 250
```

Expected on M1 Pro:
- Original: ~28s (8.9 FPS)
- Optimized: ~15-18s (14-17 FPS)
- **Improvement: 40-60% faster!**

## Memory Usage

**Original:**
- Allocates new buffers per frame
- GC runs frequently
- ~800 MB peak

**Optimized:**
- Reuses buffers from pool
- Minimal GC activity
- ~400-500 MB peak
- **50% less memory!**

## Technical Details

### Memory Pools
```go
type TensorPool struct {
    pool sync.Pool
}

// Reuse tensors
tensor := pool.Get()
// ... use tensor ...
pool.Put(tensor)  // Return for reuse
```

### Parallel Processing
```go
var wg sync.WaitGroup
for _, frame := range batch {
    wg.Add(1)
    go func(idx int) {
        defer wg.Done()
        processFrame(idx)
    }(frame)
}
wg.Wait()
```

### Direct Pixel Access
```go
// Fast: Direct buffer access
pix := img.Pix
for i := 0; i < len(pix); i += 4 {
    r := pix[i+0]
    g := pix[i+1]
    b := pix[i+2]
}

// Slow: Function call per pixel
for y := 0; y < height; y++ {
    for x := 0; x < width; x++ {
        r, g, b, _ := img.At(x, y).RGBA()  // Overhead!
    }
}
```

## Comparison

| Feature | Original | Optimized | Improvement |
|---------|----------|-----------|-------------|
| Parallelization | âŒ | âœ… All cores | +30-50% |
| Memory pooling | âŒ | âœ… sync.Pool | +10-20% |
| Buffer access | img.At() | Direct Pix | +40-60% |
| Batch processing | âŒ | âœ… Configurable | +10-15% |
| **Expected FPS** | **8.9** | **15-20** | **~2x** |

## Status

âœ… Code complete  
â³ Ready to build and test  

---

**Let's see how fast we can make Go!** ðŸš€

