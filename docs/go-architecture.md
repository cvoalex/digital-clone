# Go Optimized Frame Generation Architecture

## Overview

High-performance, Python-free frame generation using Go with ONNX Runtime.

**Performance: 21.6 FPS** (full 1280x720 frame compositing, cached)

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Go Optimized Generator (8 CPU Cores)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Command: ./bin/infer --audio talk_hb.wav --frames 250       â”‚
â”‚         â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Initialize (8 parallel ONNX sessions!)                â”‚  â”‚
â”‚  â”‚  â€¢ 8 Generator sessions (one per CPU core)            â”‚  â”‚
â”‚  â”‚  â€¢ 2 Audio encoder sessions                           â”‚  â”‚
â”‚  â”‚  â€¢ Tensor cache                                       â”‚  â”‚
â”‚  â”‚  â€¢ Memory pools                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“                                                     â”‚
â”‚  [1] Load Audio (demo/talk_hb.wav)                           â”‚
â”‚         â†“                                                     â”‚
â”‚  [2] Mel Processing (Go DSP)                                 â”‚
â”‚      â€¢ Load WAV: 718,147 samples                             â”‚
â”‚      â€¢ STFT: 3,587 frames                                    â”‚
â”‚      â€¢ Mel spectrogram generation                            â”‚
â”‚         â†“                                                     â”‚
â”‚  [3] Audio Encoding (ONNX Runtime - 2 sessions)              â”‚
â”‚      â€¢ 1,117 frames encoded                                  â”‚
â”‚      â€¢ Parallel batches                                      â”‚
â”‚      â€¢ ~9 seconds                                            â”‚
â”‚         â†“                                                     â”‚
â”‚  [4] Frame Generation (PARALLEL - 8 sessions!)               â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚      â”‚ Batch Processing (15 frames/batch)         â”‚         â”‚
â”‚      â”‚                                             â”‚         â”‚
â”‚      â”‚ For each batch (17 batches total):         â”‚         â”‚
â”‚      â”‚                                             â”‚         â”‚
â”‚      â”‚   Goroutine Pool (8 workers):              â”‚         â”‚
â”‚      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚
â”‚      â”‚   â”‚ Worker 1: Frames 1, 9, 17, 25...    â”‚ â”‚         â”‚
â”‚      â”‚   â”‚ Worker 2: Frames 2, 10, 18, 26...   â”‚ â”‚         â”‚
â”‚      â”‚   â”‚ Worker 3: Frames 3, 11, 19, 27...   â”‚ â”‚         â”‚
â”‚      â”‚   â”‚ Worker 4: Frames 4, 12, 20, 28...   â”‚ â”‚         â”‚
â”‚      â”‚   â”‚ Worker 5: Frames 5, 13, 21, 29...   â”‚ â”‚         â”‚
â”‚      â”‚   â”‚ Worker 6: Frames 6, 14, 22, 30...   â”‚ â”‚         â”‚
â”‚      â”‚   â”‚ Worker 7: Frames 7, 15, 23, 31...   â”‚ â”‚         â”‚
â”‚      â”‚   â”‚ Worker 8: Frames 8, 16, 24, 32...   â”‚ â”‚         â”‚
â”‚      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚
â”‚      â”‚                                             â”‚         â”‚
â”‚      â”‚   Per frame:                               â”‚         â”‚
â”‚      â”‚   a) Load images (roi, masked, fullbody)   â”‚         â”‚
â”‚      â”‚   b) Convert to tensor (check cache!)      â”‚         â”‚
â”‚      â”‚   c) Get ONNX session from pool            â”‚         â”‚
â”‚      â”‚   d) Run generator inference               â”‚         â”‚
â”‚      â”‚   e) Return session to pool                â”‚         â”‚
â”‚      â”‚   f) Convert tensor â†’ image                â”‚         â”‚
â”‚      â”‚   g) Composite into full frame             â”‚         â”‚
â”‚      â”‚   h) Save JPEG                             â”‚         â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â†“                                                     â”‚
â”‚  [5] Output 250 frames (1280x720)                            â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Session Pooling (Key Innovation!)

```go
// Create 8 generator sessions (one per CPU core)
generatorPool := NewSessionPool(modelPath, 8)

// Each goroutine gets its own session
session := pool.Get()        // Blocks if all busy
output := session.Run(input) // Parallel inference!
pool.Put(session)            // Return to pool

// Result: 8 inferences running simultaneously!
```

**Why this matters:**
- Without pooling: 1 session, mutex, serial = 8.9 FPS
- With pooling: 8 sessions, parallel = 21.6 FPS
- **2.4x speedup!**

## Memory Pooling

```go
type TensorPool struct {
    pool sync.Pool
}

// Reuse tensors across frames
tensor := pool.Get().([]float32)
// ... use tensor ...
pool.Put(tensor)  // Return for next frame

// Zero allocations during processing!
```

## Tensor Caching

```go
type TensorCache struct {
    cacheDir string
    cache    map[string][]float32
}

// First time: Convert image â†’ tensor
tensor := convertImage(img)
cache.Save("roi_1.jpg", tensor)

// Second time: Load from cache
tensor := cache.Load("roi_1.jpg")  // Instant!

// Result: 100% hit rate on cached runs
```

## Performance Breakdown (250 frames, cached)

| Operation | Time | % | Parallel? |
|-----------|------|---|-----------|
| Audio processing | ~9s | - | Startup only |
| Image loading | 0.5s | 4% | âœ… Yes (per goroutine) |
| Tensor conversion | 0.2s | 2% | âœ… Cached! |
| **ONNX inference** | **8s** | **70%** | âœ… 8 sessions |
| Image compositing | 1s | 9% | âœ… Yes |
| JPEG saving | 1.5s | 13% | âœ… Yes |
| Other | 0.2s | 2% | - |
| **Total** | **~11.5s** | **100%** | **21.6 FPS** |

## Why ONNX Inference is 70%

ONNX Runtime on CPU/GPU is the bottleneck:
- 8 sessions run in parallel (maximized!)
- Each takes ~1s per frame
- CPU-bound (no Neural Engine)
- Already using all cores

**Can't optimize further without:**
- NVIDIA GPU (CUDA execution)
- TensorRT (GPU-optimized)
- Different ML runtime

## Optimizations Applied

### 1. Session Pooling
- 8 parallel ONNX sessions
- True concurrent inference
- 2.4x speedup over single session

### 2. Batch Processing
- Optimal batch size: 15
- Reduces synchronization overhead
- Better scheduling

### 3. Memory Pooling
- sync.Pool for tensors
- Eliminates allocations
- Reduces GC pressure

### 4. Tensor Caching
- Disk-based cache
- 100% hit rate on repeat runs
- Saves conversion time

### 5. Direct Pixel Access
```go
pix := img.Pix  // Direct buffer
for i := 0; i < len(pix); i += 4 {
    tensor[...] = float32(pix[i])
}
// No img.At() overhead!
```

## Comparison to iOS

| Aspect | Go | iOS | Winner |
|--------|----|----|--------|
| ML Runtime | ONNX | Core ML | iOS |
| Hardware | CPU/GPU | Neural Engine | iOS |
| Parallelization | 8 workers | Unlimited | iOS |
| Platform | Any | Apple only | Go |
| Deployment | Anywhere | iOS/Mac only | Go |
| FPS | 21.6 | 48 | iOS |

**Trade-off:**
- iOS: 2x faster but Apple-only
- Go: Portable but slower on Apple hardware

## On NVIDIA GPU

With CUDA execution provider:
```go
// Use ONNX Runtime with CUDA
session := ort.NewSessionWithCUDA(...)

// Expected: 50-80 FPS (3-4x faster!)
// TensorRT: 100+ FPS (5-10x faster!)
```

Go would be FASTEST on NVIDIA hardware!

## Code Structure

```
go_optimized/
â”œâ”€â”€ cmd/infer/main.go          CLI entry point
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ parallel/
â”‚   â”‚   â”œâ”€â”€ generator.go       Main generator with session pool
â”‚   â”‚   â””â”€â”€ session_pool.go    ONNX session management
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â””â”€â”€ processor.go       Batch processing logic
â”‚   â”œâ”€â”€ pool/
â”‚   â”‚   â””â”€â”€ pool.go            Memory pools
â”‚   â””â”€â”€ cache/
â”‚       â””â”€â”€ tensor_cache.go    Tensor caching
â””â”€â”€ bin/infer                   Compiled binary (3.4 MB)
```

## Usage

```bash
# First run (builds cache)
DYLD_LIBRARY_PATH=/opt/homebrew/lib ./bin/infer \
  --audio demo/talk_hb.wav \
  --frames 250 \
  --batch 15

# Output: 15.7 FPS (building cache)

# Second run (uses cache)
# Output: 21.6 FPS (100% cache hits!)
```

## Deployment

**Advantages:**
- Single 3.4 MB binary
- No Python runtime
- Runs on Linux, macOS, Windows
- Only needs ONNX Runtime library

**Use cases:**
- Server deployments
- Docker containers
- Edge devices
- Non-Apple hardware

---

**Go is maximally optimized for ONNX Runtime - 21.6 FPS is near the theoretical limit on CPU!** ğŸš€

For 2-5x faster, use NVIDIA GPU with CUDA/TensorRT.

