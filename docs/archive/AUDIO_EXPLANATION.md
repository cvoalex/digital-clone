# Audio Usage Explanation

## What Audio Are We Using?

### The Sanders Package Has:

1. **`aud.wav`** (1.3 MB)
   - Original audio file
   - Used in final video (merged with generated frames)
   - NOT used for frame generation

2. **`aud_ave.npy`** (1.0 MB)  
   - **PRE-COMPUTED audio features**
   - Shape: (522, 512)
   - Already processed through the audio encoder
   - **This is what we use for frame generation!**

## What We're NOT Doing

âŒ Running the audio encoder on `aud.wav`  
âŒ Processing audio in real-time  
âŒ Using the audio encoder ONNX model  

## What We ARE Doing

âœ… Using **pre-computed** audio features (`aud_ave.npy`)  
âœ… These were created by someone else ahead of time  
âœ… We just load them and use them directly  

## The Flow

```
Audio Processing (ALREADY DONE):
  aud.wav â†’ [Audio Encoder] â†’ aud_ave.npy (522 Ã— 512 features)
                                    â†“
                              [STORED IN PACKAGE]
                                    â†“
Frame Generation (WHAT WE'RE DOING):
  aud_ave.npy â†’ [Reshape] â†’ (1, 32, 16, 16) per frame
       +
  Pre-cut frames â†’ [U-Net Model] â†’ Generated frames
                                    â†“
  Generated frames + aud.wav â†’ [ffmpeg] â†’ Final video
```

## Why This Works

The Sanders package is **complete** - it includes:
- âœ… Pre-processed audio features (no need to run encoder!)
- âœ… Pre-cut image frames (no need to crop!)
- âœ… U-Net ONNX model (no need for PyTorch!)
- âœ… Original audio for final video

## What Each File Does

| File | Purpose | Used When |
|------|---------|-----------|
| `aud.wav` | Original audio | Final video assembly (ffmpeg) |
| `aud_ave.npy` | Audio features | Frame generation (ONNX input) |
| `aud_ave.bin` | Audio features (binary) | Go/Swift (same data, different format) |
| `models/audio_encoder.onnx` | Audio encoder | NOT USED (features pre-computed) |
| `models/generator.onnx` | U-Net model | Frame generation (main model) |

## Current Implementation

### Python:
```python
# Load pre-computed features (NOT running encoder)
audio_feats = np.load('aud_ave.npy')  # (522, 512)

# For each frame
audio_frame = audio_feats[i]  # (512,)

# Reshape to match model input
audio_reshaped = tile_and_reshape(audio_frame)  # (1, 32, 16, 16)

# Run U-Net (NOT audio encoder)
output = unet_model.run(image_input, audio_reshaped)
```

### Go:
```go
// Load pre-computed features (NOT running encoder)
audioFeats := loadBinary('aud_ave.bin')  // 522 Ã— 512

// For each frame
audioFrame := audioFeats[i]  // 512 floats

// Reshape to match model input
audioReshaped := reshape(audioFrame)  // (1, 32, 16, 16)

// Run U-Net (NOT audio encoder)
output := unetModel.Run(imageInput, audioReshaped)
```

## Why We Don't Need Audio Encoder

The Sanders package **already ran** the audio encoder:

```
THEY DID (pre-packaged):
  aud.wav â†’ [Audio Encoder ONNX] â†’ aud_ave.npy âœ…

WE DO (frame generation):
  aud_ave.npy â†’ [Reshape] â†’ [U-Net] â†’ Frames âœ…
```

## Two Separate Models

1. **Audio Encoder** (`audio_encoder.onnx`)
   - Input: Mel spectrograms
   - Output: 512-dim features
   - **Not used** - features pre-computed

2. **U-Net Generator** (`generator.onnx`)  
   - Input: 6-channel image + audio features
   - Output: Generated lip region
   - **This is what we use!**

## Summary

**Audio being used:**
- âœ… Pre-computed features from `aud_ave.npy` (for frame generation)
- âœ… Original `aud.wav` (for final video audio track)

**Models being used:**
- âœ… U-Net generator.onnx (for frame generation)
- âŒ Audio encoder.onnx (NOT used - features pre-computed)

**Audio encoder:**
- âŒ Not running it
- âœ… Using its pre-computed outputs

---

**TL;DR:** We're using **pre-computed audio features** from the Sanders package. No audio encoding needed - just frame generation! ğŸ¬

